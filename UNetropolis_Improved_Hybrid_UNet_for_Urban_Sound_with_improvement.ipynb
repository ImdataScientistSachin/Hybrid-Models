{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # Suppress TensorFlow logging messages (e.g., warnings about GPU setup)\n",
        "\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa # Library for audio analysis\n",
        "import random\n",
        "import matplotlib.pyplot as plt # For plotting\n",
        "from IPython.display import Audio # For playing audio in Jupyter/IPython environments\n",
        "\n",
        "# Import necessary Keras components from TensorFlow\n",
        "from tensorflow.keras.utils import Sequence # Base class for Keras data generators\n",
        "from tensorflow.keras import Input, Model # For defining functional API models\n",
        "from tensorflow.keras import layers, callbacks # Core layers and callback functions\n",
        "from tensorflow.keras.layers import (\n",
        "    Cropping2D, Lambda, Conv2D, Dropout, MaxPooling2D, Conv2DTranspose,\n",
        "    Concatenate, GlobalAveragePooling2D, Activation, BatchNormalization, Dense\n",
        ") # Specific layers used in the model\n",
        "from tensorflow.keras.optimizers import Adam # Optimizer for model training\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau # Learning rate scheduler callback\n",
        "\n",
        "# Import metrics and visualization tools from scikit-learn\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "\n",
        "# --- Directory Setup ---\n",
        "# Define a list of directories to be created in the Kaggle working environment.\n",
        "# These directories are used for storing data, source code, models, and documentation.\n",
        "dirs = [\n",
        "    \"data/UrbanSound8K\", # Placeholder for processed data or temporary files\n",
        "    \"data/masks\",       # Placeholder for mask-related data\n",
        "    \"notebooks\",        # For Jupyter notebooks (though this is already a notebook)\n",
        "    \"src\",              # For Python source files (like datagen.py)\n",
        "    \"models\",           # For saving trained Keras models\n",
        "    \"docs\"              # For documentation or reports\n",
        "]\n",
        "# Create each directory if it does not already exist.\n",
        "for d in dirs:\n",
        "    os.makedirs(f\"/kaggle/working/{d}\", exist_ok=True)\n",
        "print(\"Working directories created/ensured:\", dirs)\n",
        "\n",
        "# --- Data Loading and Initial Exploration ---\n",
        "# List contents of the Kaggle input directory and the specific UrbanSound8K dataset directory\n",
        "# to verify that the dataset is mounted correctly.\n",
        "print(\"\\n--- Listing Dataset Contents ---\")\n",
        "print(\"Kaggle input directory:\", os.listdir(\"/kaggle/input/\"))\n",
        "print(\"UrbanSound8K dataset directory:\", os.listdir(\"/kaggle/input/urbansound8k/\"))\n",
        "\n",
        "print(\"\\n--- Loading Metadata ---\")\n",
        "# Define the path to the UrbanSound8K metadata CSV file.\n",
        "metadata_path = \"/kaggle/input/urbansound8k/UrbanSound8K.csv\"\n",
        "# Load the metadata into a pandas DataFrame.\n",
        "df = pd.read_csv(metadata_path)\n",
        "print(\"Metadata head:\\n\", df.head()) # Display the first few rows of the DataFrame\n",
        "\n",
        "print(\"\\n--- Class Distribution ---\")\n",
        "# Print the unique class labels present in the dataset.\n",
        "print(\"Unique classes:\", df['class'].unique())\n",
        "# Print the count of samples for each class to check for class imbalance.\n",
        "print(\"Class counts:\\n\", df['class'].value_counts())\n",
        "\n",
        "print(\"\\n--- Fold Distribution ---\")\n",
        "# Print the count of samples per fold to understand the data distribution across folds.\n",
        "print(\"Fold counts:\\n\", df['fold'].value_counts())\n",
        "\n",
        "print(\"\\n--- Missing Values Check ---\")\n",
        "# Check for any missing values across the entire DataFrame.\n",
        "print(\"Any missing values in metadata:\", df.isna().sum().sum())\n",
        "\n",
        "\n",
        "# --- Audio Sample Loading and Visualization ---\n",
        "print(\"\\n--- Audio Sample Demonstration ---\")\n",
        "# Select a random audio sample's metadata from the DataFrame for demonstration.\n",
        "sample_meta = df.sample(1, random_state=7).iloc[0]\n",
        "# Construct the full file path for the selected audio sample.\n",
        "file_path = f\"/kaggle/input/urbansound8k/fold{sample_meta['fold']}/{sample_meta['slice_file_name']}\"\n",
        "\n",
        "# Load the audio file using librosa.\n",
        "# `sr=22050` resamples the audio to 22050 Hz.\n",
        "# `duration=4` loads only the first 4 seconds of the audio.\n",
        "y, sr = librosa.load(file_path, sr=22050, duration=4)\n",
        "print(f\"Loaded '{sample_meta['slice_file_name']}'; Class: {sample_meta['class']} | Sample Rate: {sr} | Samples: {len(y)}\")\n",
        "\n",
        "# Play audio directly in the Jupyter/IPython environment.\n",
        "print(\"Playing audio sample:\")\n",
        "Audio(y, rate=sr) # Using IPython.display.Audio directly\n",
        "\n",
        "# Display & plot audio waveform using matplotlib and librosa.display.\n",
        "plt.figure(figsize=(12, 3))\n",
        "librosa.display.waveshow(y, sr=sr) # Plot the waveform\n",
        "plt.title(f\"Waveform | {sample_meta['class']} ({sample_meta['slice_file_name']})\")\n",
        "plt.xlabel(\"Time (s)\")\n",
        "plt.ylabel(\"Amplitude\")\n",
        "plt.show()\n",
        "\n",
        "# --- Spectrogram Visualization Function ---\n",
        "def show_urbansound8k_samples(\n",
        "    df,\n",
        "    audio_dir=\"/kaggle/input/urbansound8k\",\n",
        "    n_samples=6,\n",
        "    random_state=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Displays mel spectrograms for a given number of random UrbanSound8K samples.\n",
        "    Includes min-max normalization for consistent visualization.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The metadata DataFrame.\n",
        "        audio_dir (str): Base directory where audio files are stored.\n",
        "        n_samples (int): Number of random samples to display.\n",
        "        random_state (int, optional): Seed for reproducibility. Defaults to None.\n",
        "    \"\"\"\n",
        "    # Sample rows from the DataFrame to visualize.\n",
        "    sample_df = df.sample(n_samples, random_state=random_state)\n",
        "    plt.figure(figsize=(6 * n_samples, 6)) # Adjust figure size based on number of samples\n",
        "    for i, row in enumerate(sample_df.itertuples()): # Iterate through sampled rows\n",
        "        file_path = f\"{audio_dir}/fold{row.fold}/{row.slice_file_name}\"\n",
        "        try:\n",
        "            # Load audio, resample, and trim/pad to a fixed duration.\n",
        "            y, sr = librosa.load(file_path, sr=22050, duration=4)\n",
        "            if len(y) < 4 * 22050:\n",
        "                y = np.pad(y, (0, 4 * 22050 - len(y)), mode='constant')\n",
        "\n",
        "            # Compute mel spectrogram and convert to decibels.\n",
        "            mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
        "            mel_db = librosa.power_to_db(mel, ref=np.max)\n",
        "\n",
        "            # --- IMPROVEMENT: Min-Max Normalization for Visualization ---\n",
        "            # Normalize spectrograms to a [0, 1] range for consistent color mapping\n",
        "            # across different spectrograms. Add a small epsilon to prevent division by zero.\n",
        "            mel_db_normalized = (mel_db - mel_db.min()) / (mel_db.max() - mel_db.min() + 1e-8)\n",
        "\n",
        "            # Plot the normalized mel spectrogram.\n",
        "            plt.subplot(1, n_samples, i+1) # Create subplot for each sample\n",
        "            plt.imshow(mel_db_normalized, origin='lower', aspect='auto', cmap='magma') # Use normalized data\n",
        "            # Robustly get the class label for the title.\n",
        "            label = getattr(row, 'class', getattr(row, 'classID', 'Unknown'))\n",
        "            plt.title(f\"Class: {label}\")\n",
        "            plt.axis('off') # Turn off axes for cleaner spectrogram display\n",
        "        except Exception as e:\n",
        "            # Handle errors during loading or plotting for individual files.\n",
        "            print(f\"Error loading or plotting {file_path}: {e}\")\n",
        "            plt.subplot(1, n_samples, i+1)\n",
        "            plt.axis('off')\n",
        "            plt.title(\"Error\")\n",
        "    plt.suptitle(f\"Random {n_samples} UrbanSound8K Spectrograms\", fontsize=18) # Main title for the figure\n",
        "    plt.tight_layout() # Adjust subplot parameters for a tight layout.\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\n--- Visualizing Sample Spectrograms ---\")\n",
        "show_urbansound8k_samples(df, audio_dir=\"/kaggle/input/urbansound8k\", n_samples=6, random_state=42)\n",
        "\n",
        "\n",
        "# --- CustomDataGen Class Definition (Improved) ---\n",
        "# Create the /kaggle/working/src/ Directory to store the custom data generator.\n",
        "os.makedirs(\"/kaggle/working/src\", exist_ok=True)\n",
        "\n",
        "# Write the CustomDataGen class code to a Python file.\n",
        "# This allows it to be imported as a module.\n",
        "with open(\"/kaggle/working/src/datagen.py\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "import numpy as np\n",
        "import librosa\n",
        "import tensorflow as tf # Required for tf.keras.utils.Sequence\n",
        "\n",
        "class CustomDataGen(tf.keras.utils.Sequence):\n",
        "    \\\"\\\"\\\"\n",
        "    A custom Keras data generator for the UrbanSound8K dataset.\n",
        "    It loads audio, converts to mel spectrograms, applies padding,\n",
        "    and supports optional mask overlay augmentation.\n",
        "    Spectrograms are min-max normalized before being returned.\n",
        "    \\\"\\\"\\\"\n",
        "    def __init__(self, df, audio_dir, batch_size=8, shuffle=True, n_mels=128, duration=4, sr=22050, mask_overlay_df=None):\n",
        "        self.df = df.reset_index(drop=True) # Reset index to ensure consistent indexing\n",
        "        self.audio_dir = audio_dir # Base directory for audio files\n",
        "        self.batch_size = batch_size # Number of samples per batch\n",
        "        self.shuffle = shuffle # Whether to shuffle data at the end of each epoch\n",
        "        self.n_mels = n_mels # Number of mel bands for spectrogram\n",
        "        self.duration = duration # Duration of audio clips in seconds\n",
        "        self.sr = sr # Target sample rate for audio\n",
        "        self.mask_overlay_df = mask_overlay_df # DataFrame for overlay augmentation samples\n",
        "        self.indexes = np.arange(len(self.df)) # Array of indices for the dataset\n",
        "        self.on_epoch_end() # Initial shuffle of indexes\n",
        "\n",
        "    def __len__(self):\n",
        "        # Returns the number of batches per epoch.\n",
        "        # np.floor ensures we only return full batches.\n",
        "        return int(np.floor(len(self.df) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Generates one batch of data given an index.\n",
        "        # Selects indices for the current batch.\n",
        "        batch_indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        # Retrieves the corresponding rows from the DataFrame.\n",
        "        batch = self.df.iloc[batch_indexes]\n",
        "        # Generates the actual data (mel spectrograms and labels) for the batch.\n",
        "        X, y = self.__data_generation(batch)\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        # Updates indexes after each epoch. This method is called by Keras after each epoch.\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes) # Shuffle indices for the next epoch\n",
        "\n",
        "    def __load_audio(self, file_path):\n",
        "        # Loads an audio file, resamples it, and pads/trims to a fixed duration.\n",
        "        y, _ = librosa.load(file_path, sr=self.sr, duration=self.duration)\n",
        "        # Pad audio if its length is less than the target duration.\n",
        "        if len(y) < int(self.sr * self.duration):\n",
        "            y = np.pad(y, (0, int(self.sr * self.duration - len(y))), \"constant\")\n",
        "        return y\n",
        "\n",
        "    def __mel_spectrogram(self, y):\n",
        "        # Converts an audio waveform into a mel spectrogram.\n",
        "        mel = librosa.feature.melspectrogram(y=y, sr=self.sr, n_mels=self.n_mels)\n",
        "        # Convert power spectrogram to decibel (dB) scale.\n",
        "        mel_db = librosa.power_to_db(mel, ref=np.max)\n",
        "        return mel_db\n",
        "\n",
        "    def __overlay_augment(self, base_audio, overlay_audio, snr_db=10):\n",
        "        # Overlays a `base_audio` with an `overlay_audio` at a specified Signal-to-Noise Ratio (SNR).\n",
        "        rms_base = np.sqrt(np.mean(base_audio ** 2)) # RMS of base audio\n",
        "        rms_overlay = np.sqrt(np.mean(overlay_audio ** 2)) # RMS of overlay audio\n",
        "\n",
        "        if rms_overlay == 0: # Avoid division by zero if overlay is silent\n",
        "            return base_audio\n",
        "\n",
        "        # Calculate desired RMS of overlay based on SNR.\n",
        "        desired_rms_overlay = rms_base / (10**(snr_db / 20))\n",
        "        # Scale the overlay audio to achieve the desired RMS.\n",
        "        scaled_overlay = overlay_audio * (desired_rms_overlay / (rms_overlay + 1e-8))\n",
        "        # Mix the base and scaled overlay audio.\n",
        "        mixed = base_audio + scaled_overlay\n",
        "        mixed = np.clip(mixed, -1.0, 1.0) # Clip to ensure valid audio amplitude range [-1.0, 1.0]\n",
        "        return mixed\n",
        "\n",
        "    def __data_generation(self, batch_df):\n",
        "        # Generates preprocessed data (mel spectrograms) and labels for a given batch DataFrame.\n",
        "        X = [] # List to store input features (spectrograms)\n",
        "        y = [] # List to store target labels\n",
        "        for _, row in batch_df.iterrows():\n",
        "            file_path = f\"{self.audio_dir}/fold{row['fold']}/{row['slice_file_name']}\"\n",
        "            base_audio = self.__load_audio(file_path)\n",
        "\n",
        "            # Apply mask overlay augmentation with a 50% probability if mask_overlay_df is provided.\n",
        "            if self.mask_overlay_df is not None and np.random.rand() < 0.5:\n",
        "                overlay_sample = self.mask_overlay_df.sample(1).iloc[0] # Pick a random overlay sample\n",
        "                overlay_path = f\"{self.audio_dir}/fold{overlay_sample['fold']}/{overlay_sample['slice_file_name']}\"\n",
        "                overlay_audio = self.__load_audio(overlay_path)\n",
        "                base_audio = self.__overlay_augment(base_audio, overlay_audio, snr_db=10)\n",
        "\n",
        "            mel_spec = self.__mel_spectrogram(base_audio)\n",
        "\n",
        "            # --- IMPROVEMENT: Min-Max Normalization for Mel Spectrograms ---\n",
        "            # Normalize spectrograms to a [0, 1] range. This is crucial for neural network performance\n",
        "            # as it helps with gradient stability and faster convergence.\n",
        "            # Add a small epsilon (1e-8) to the denominator to prevent division by zero for flat spectrograms.\n",
        "            mel_spec_normalized = (mel_spec - np.min(mel_spec)) / (np.max(mel_spec) - np.min(mel_spec) + 1e-8)\n",
        "\n",
        "            # Add a channel dimension for Keras (expected format: batch, height, width, channels).\n",
        "            # For grayscale images/spectrograms, channels=1.\n",
        "            mel_spec_final = np.expand_dims(mel_spec_normalized, axis=-1)\n",
        "            X.append(mel_spec_final)\n",
        "            y.append(row['classID']) # Append the class ID as the label\n",
        "\n",
        "        # Convert lists of features and labels to numpy arrays with appropriate dtypes.\n",
        "        # float32 for input features, int64 for labels (for sparse_categorical_crossentropy).\n",
        "        return np.array(X, dtype=np.float32), np.array(y, dtype=np.int64)\n",
        "\"\"\")\n",
        "\n",
        "# Re-import CustomDataGen to ensure the latest version (with improvements) is used.\n",
        "sys.path.append(\"/kaggle/working/src/\")\n",
        "from datagen import CustomDataGen\n",
        "\n",
        "# --- Test the Data Generator ---\n",
        "print(\"\\n--- Testing CustomDataGen ---\")\n",
        "# Create a small DataFrame for mask overlay samples (optional augmentation).\n",
        "mask_overlay_df = df[df['fold'] == 1].sample(20, random_state=42)\n",
        "# Instantiate the CustomDataGen for demonstration.\n",
        "datagen = CustomDataGen(\n",
        "    df=df,\n",
        "    audio_dir=\"/kaggle/input/urbansound8k\",\n",
        "    batch_size=8,\n",
        "    n_mels=128,\n",
        "    duration=4,\n",
        "    sr=22050,\n",
        "    mask_overlay_df=mask_overlay_df # Pass the mask overlay DataFrame\n",
        ")\n",
        "\n",
        "# Retrieve a single batch of data from the generator.\n",
        "X_batch, y_batch = datagen[0]\n",
        "print(\"X_batch shape:\", X_batch.shape) # Expected: (batch_size, n_mels, time_steps, 1)\n",
        "print(\"y_batch:\", y_batch) # Expected: (batch_size,) of integer class IDs\n",
        "\n",
        "# Plot the first spectrogram from the batch to visualize the normalized input.\n",
        "plt.figure(figsize=(10, 4))\n",
        "# Transpose the spectrogram for correct orientation (time on x-axis, mel bands on y-axis).\n",
        "plt.imshow(X_batch[0][:, :, 0].T, aspect='auto', origin='lower', cmap='magma')\n",
        "plt.title(f\"Normalized Spectrogram for ClassID: {y_batch[0]}\")\n",
        "plt.xlabel(\"Time bins\")\n",
        "plt.ylabel(\"Mel bands\")\n",
        "# Note: Colorbar might not accurately reflect dB after normalization to [0,1].\n",
        "plt.colorbar(format=\"%+2.0f dB\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Check the original mel_db shape for a single sample (for comparison with model input shape).\n",
        "y_test, sr_test = librosa.load(file_path, sr=22050, duration=4)\n",
        "mel_test = librosa.feature.melspectrogram(y=y_test, sr=sr_test, n_mels=128)\n",
        "mel_db_test = librosa.power_to_db(mel_test)\n",
        "print(\"Original mel_db shape for a single sample:\", mel_db_test.shape)\n",
        "\n",
        "\n",
        "# --- Double UNet/Hybrid Mask-aware Model Definition (Improved) ---\n",
        "def crop_to_match(encoder_tensor, decoder_tensor):\n",
        "    \"\"\"\n",
        "    Crops the encoder tensor to match the spatial dimensions of the decoder tensor.\n",
        "    This is essential for skip connections in UNet architectures, where feature maps\n",
        "    from the encoder are concatenated with upsampled feature maps from the decoder.\n",
        "\n",
        "    Args:\n",
        "        encoder_tensor (tf.Tensor): Feature map from the encoder path.\n",
        "        decoder_tensor (tf.Tensor): Feature map from the decoder path (upsampled).\n",
        "\n",
        "    Returns:\n",
        "        tf.Tensor: Cropped encoder tensor.\n",
        "    \"\"\"\n",
        "    # Calculate the amount to crop from height and width.\n",
        "    # max(0, ...) ensures no negative cropping (i.e., no padding).\n",
        "    crop_height = encoder_tensor.shape[1] - decoder_tensor.shape[1]\n",
        "    crop_width = encoder_tensor.shape[2] - decoder_tensor.shape[2]\n",
        "\n",
        "    # Define the cropping dimensions for Cropping2D layer.\n",
        "    cropping = ((0, max(0, crop_height)), (0, max(0, crop_width)))\n",
        "\n",
        "    # Apply Cropping2D only if cropping is necessary.\n",
        "    if crop_height != 0 or crop_width != 0:\n",
        "        encoder_tensor = Cropping2D(cropping=cropping)(encoder_tensor)\n",
        "    return encoder_tensor\n",
        "\n",
        "def unet_block(inputs, filters, kernel_size=(3, 3), dropout=0.3):\n",
        "    \"\"\"\n",
        "    Defines a single UNet encoder-decoder block with improved architecture.\n",
        "    Each convolutional block now includes two Conv2D layers followed by BatchNormalization\n",
        "    and ReLU activation, which generally improves training stability and performance.\n",
        "\n",
        "    Args:\n",
        "        inputs (tf.Tensor): The input tensor to the UNet block.\n",
        "        filters (int): The base number of filters for the convolutional layers.\n",
        "        kernel_size (tuple): Size of the convolutional kernels.\n",
        "        dropout (float): Dropout rate for regularization.\n",
        "\n",
        "    Returns:\n",
        "        tf.Tensor: The output tensor of the UNet block.\n",
        "    \"\"\"\n",
        "    # --- Encoder Path ---\n",
        "    # Consists of two convolutional layers, each followed by BatchNormalization and ReLU,\n",
        "    # then Dropout, and finally MaxPooling for downsampling.\n",
        "\n",
        "    # First convolutional block in the encoder\n",
        "    c1 = Conv2D(filters, kernel_size, padding='same')(inputs)\n",
        "    c1 = BatchNormalization()(c1) # IMPROVEMENT: Added BatchNormalization for stable training\n",
        "    c1 = Activation('relu')(c1)\n",
        "\n",
        "    c1 = Conv2D(filters, kernel_size, padding='same')(c1)\n",
        "    c1 = BatchNormalization()(c1) # IMPROVEMENT: Added BatchNormalization\n",
        "    c1 = Activation('relu')(c1)\n",
        "    c1 = Dropout(dropout)(c1) # Dropout for regularization\n",
        "    p1 = MaxPooling2D((2, 2))(c1) # Downsample by 2x2\n",
        "\n",
        "    # Second convolutional block in the encoder\n",
        "    c2 = Conv2D(filters*2, kernel_size, padding='same')(p1)\n",
        "    c2 = BatchNormalization()(c2) # IMPROVEMENT: Added BatchNormalization\n",
        "    c2 = Activation('relu')(c2)\n",
        "\n",
        "    c2 = Conv2D(filters*2, kernel_size, padding='same')(c2)\n",
        "    c2 = BatchNormalization()(c2) # IMPROVEMENT: Added BatchNormalization\n",
        "    c2 = Activation('relu')(c2)\n",
        "    c2 = Dropout(dropout)(c2) # Dropout for regularization\n",
        "    p2 = MaxPooling2D((2, 2))(c2) # Downsample by 2x2\n",
        "\n",
        "    # --- Bottleneck ---\n",
        "    # The deepest part of the UNet, capturing the most abstract features.\n",
        "    b1 = Conv2D(filters*4, kernel_size, padding='same')(p2)\n",
        "    b1 = BatchNormalization()(b1) # IMPROVEMENT: Added BatchNormalization\n",
        "    b1 = Activation('relu')(b1)\n",
        "\n",
        "    b1 = Conv2D(filters*4, kernel_size, padding='same')(b1)\n",
        "    b1 = BatchNormalization()(b1) # IMPROVEMENT: Added BatchNormalization\n",
        "    b1 = Activation('relu')(b1)\n",
        "    b1 = Dropout(dropout)(b1) # Dropout for regularization\n",
        "\n",
        "    # --- Decoder Path ---\n",
        "    # Upsamples feature maps and concatenates them with corresponding encoder feature maps (skip connections).\n",
        "    # Each upsampling block consists of Conv2DTranspose, Concatenation, and two Conv2D layers\n",
        "    # with BatchNormalization and ReLU.\n",
        "\n",
        "    # First upsampling block (from bottleneck to c2 level)\n",
        "    u1 = Conv2DTranspose(filters*2, (2, 2), strides=(2, 2), padding='same')(b1) # Upsample by 2x2\n",
        "    c2_cropped = crop_to_match(c2, u1) # Crop encoder feature map to match upsampled decoder map\n",
        "    u1 = Concatenate()([u1, c2_cropped]) # Concatenate for skip connection\n",
        "\n",
        "    u1 = Conv2D(filters*2, kernel_size, padding='same')(u1)\n",
        "    u1 = BatchNormalization()(u1) # IMPROVEMENT: Added BatchNormalization\n",
        "    u1 = Activation('relu')(u1)\n",
        "\n",
        "    u1 = Conv2D(filters*2, kernel_size, padding='same')(u1)\n",
        "    u1 = BatchNormalization()(u1) # IMPROVEMENT: Added BatchNormalization\n",
        "    u1 = Activation('relu')(u1)\n",
        "\n",
        "    # Second upsampling block (from u1 to c1 level)\n",
        "    u2 = Conv2DTranspose(filters, (2, 2), strides=(2, 2), padding='same')(u1) # Upsample by 2x2\n",
        "    c1_cropped = crop_to_match(c1, u2) # Crop encoder feature map to match upsampled decoder map\n",
        "    u2 = Concatenate()([u2, c1_cropped]) # Concatenate for skip connection\n",
        "\n",
        "    u2 = Conv2D(filters, kernel_size, padding='same')(u2)\n",
        "    u2 = BatchNormalization()(u2) # IMPROVEMENT: Added BatchNormalization\n",
        "    u2 = Activation('relu')(u2)\n",
        "\n",
        "    u2 = Conv2D(filters, kernel_size, padding='same')(u2)\n",
        "    u2 = BatchNormalization()(u2) # IMPROVEMENT: Added BatchNormalization\n",
        "    u2 = Activation('relu')(u2)\n",
        "\n",
        "    return u2\n",
        "\n",
        "def build_double_unet(input_shape, num_classes, dropout=0.3):\n",
        "    \"\"\"\n",
        "    Builds the Double UNet model for urban sound classification.\n",
        "    This architecture stacks two UNet-like encoder-decoder paths.\n",
        "\n",
        "    Args:\n",
        "        input_shape (tuple): Shape of the input spectrogram (height, width, channels).\n",
        "        num_classes (int): Number of output classes for classification.\n",
        "        dropout (float): Dropout rate to apply in UNet blocks.\n",
        "\n",
        "    Returns:\n",
        "        tf.keras.Model: The compiled Keras model.\n",
        "    \"\"\"\n",
        "    inp = Input(shape=input_shape) # Define the input layer\n",
        "\n",
        "    # First UNet block: Processes the raw input spectrogram.\n",
        "    block1 = unet_block(inp, filters=32, dropout=dropout)\n",
        "\n",
        "    # Second UNet block: Processes the output of the first UNet block.\n",
        "    # This \"double UNet\" structure aims to refine features or handle more complex\n",
        "    # hierarchical patterns, potentially useful for \"mask-aware\" processing.\n",
        "    block2 = unet_block(block1, filters=32, dropout=dropout)\n",
        "\n",
        "    # --- IMPROVEMENT: Classification Head with Dense Layer ---\n",
        "    # Instead of a 1x1 Conv2D followed by GlobalAveragePooling, a more typical\n",
        "    # classification head involves GlobalAveragePooling followed by one or more\n",
        "    # Dense layers. This allows the model to learn complex non-linear mappings\n",
        "    # from the pooled features to the final class probabilities.\n",
        "\n",
        "    # Apply Global Average Pooling to reduce spatial dimensions to 1x1,\n",
        "    # resulting in a feature vector for each sample in the batch.\n",
        "    pooled_features = GlobalAveragePooling2D()(block2)\n",
        "\n",
        "    # Add a Dense layer for classification.\n",
        "    # The number of units equals `num_classes`, and 'softmax' activation\n",
        "    # is used for multi-class classification, outputting probabilities for each class.\n",
        "    classification_output = Dense(num_classes, activation='softmax', name='output')(pooled_features)\n",
        "\n",
        "    # Define the Keras Model with the specified input and output.\n",
        "    model = Model(inputs=inp, outputs=classification_output)\n",
        "    return model\n",
        "\n",
        "# --- Train/Validation Splits ---\n",
        "print(\"\\n--- Preparing Data Splits ---\")\n",
        "metadata_path = \"/kaggle/input/urbansound8k/UrbanSound8K.csv\"\n",
        "audio_dir = \"/kaggle/input/urbansound8k\"\n",
        "\n",
        "df = pd.read_csv(metadata_path)\n",
        "\n",
        "# Define the fold to be used for validation (e.g., fold 1).\n",
        "# This ensures a consistent split for evaluation.\n",
        "val_fold = 1\n",
        "train_df = df[df['fold'] != val_fold].reset_index(drop=True) # Training data: all folds except validation fold\n",
        "val_df = df[df['fold'] == val_fold].reset_index(drop=True)   # Validation data: samples from the validation fold\n",
        "\n",
        "# Optional: Create a DataFrame for mask overlay samples.\n",
        "# These samples will be randomly overlaid on training data for augmentation.\n",
        "mask_overlay_df = train_df.sample(20, random_state=42)\n",
        "\n",
        "batch_size = 16 # Define the batch size for training and validation generators.\n",
        "\n",
        "# Instantiate the CustomDataGen for training and validation datasets.\n",
        "train_gen = CustomDataGen(train_df, audio_dir=audio_dir, batch_size=batch_size, shuffle=True, mask_overlay_df=mask_overlay_df)\n",
        "val_gen = CustomDataGen(val_df, audio_dir=audio_dir, batch_size=batch_size, shuffle=False, mask_overlay_df=None) # No augmentation for validation\n",
        "\n",
        "# Inspect the input shape from one batch of the training generator.\n",
        "# This shape is used to define the input layer of the Keras model.\n",
        "X_sample, y_sample = train_gen[0]\n",
        "input_shape = X_sample.shape[1:]  # (n_mels, time_steps, 1) - excludes the batch dimension\n",
        "num_classes = 10  # UrbanSound8K dataset has 10 distinct classes\n",
        "\n",
        "print(f\"Model Input Shape: {input_shape}\")\n",
        "print(f\"Number of Classes: {num_classes}\")\n",
        "\n",
        "# Build the Double UNet model with the determined input shape and number of classes.\n",
        "model = build_double_unet(input_shape=input_shape, num_classes=num_classes, dropout=0.3)\n",
        "model.summary() # Print a summary of the model architecture, including layer types and output shapes.\n",
        "\n",
        "# --- Model Compilation ---\n",
        "# Configure the model for training.\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001), # IMPROVEMENT: Explicitly set initial learning rate for Adam optimizer.\n",
        "                                         # Adam is a good default, but fine-tuning LR can help.\n",
        "    loss='sparse_categorical_crossentropy', # Appropriate loss function for multi-class classification\n",
        "                                            # with integer labels (not one-hot encoded).\n",
        "    metrics=['accuracy'] # Metric to monitor during training.\n",
        ")\n",
        "\n",
        "# --- Callbacks ---\n",
        "# Define Keras callbacks to control training behavior.\n",
        "# EarlyStopping: Stops training if validation loss doesn't improve for 'patience' epochs.\n",
        "#                `restore_best_weights=True` loads the weights from the best epoch.\n",
        "es = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
        "# ModelCheckpoint: Saves the model (or weights) when validation loss is at its minimum.\n",
        "mc = callbacks.ModelCheckpoint(\"/kaggle/working/models/double_unet_best.keras\",\n",
        "                                    save_best_only=True, monitor='val_loss', verbose=1)\n",
        "# IMPROVEMENT: ReduceLROnPlateau: Reduces the learning rate when a metric (val_loss) has stopped improving.\n",
        "#              This helps the model converge better in later stages of training.\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n",
        "\n",
        "\n",
        "# --- Model Training ---\n",
        "print(\"\\n--- Starting Model Training ---\")\n",
        "# Train the model using the data generators.\n",
        "# `steps_per_epoch` and `validation_steps` are explicitly provided for clarity\n",
        "# and robustness with custom Sequence generators.\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=20, # Maximum number of epochs to train\n",
        "    steps_per_epoch=len(train_gen), # Number of batches per training epoch\n",
        "    validation_steps=len(val_gen), # Number of batches per validation epoch\n",
        "    callbacks=[es, mc, reduce_lr], # List of callbacks to apply during training\n",
        "    verbose=2 # Verbosity mode (1 = progress bar, 2 = one line per epoch)\n",
        ")\n",
        "\n",
        "# --- Plotting Training History ---\n",
        "print(\"\\n--- Plotting Training History ---\")\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot training and validation accuracy over epochs.\n",
        "plt.subplot(1, 2, 1) # 1 row, 2 columns, first plot\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
        "plt.title('Double UNet Model Accuracy per Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Plot training and validation loss over epochs.\n",
        "plt.subplot(1, 2, 2) # 1 row, 2 columns, second plot\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.title('Double UNet Model Loss per Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout() # Adjust layout to prevent overlapping titles/labels\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# --- Confusion Matrix for Validation Results (Improved) ---\n",
        "print(\"\\n--- Generating Confusion Matrix ---\")\n",
        "\n",
        "# IMPROVEMENT: Initialize val_preds and val_truth ONCE before the loop.\n",
        "# This ensures that predictions and true labels are collected from all validation batches.\n",
        "val_preds, val_truth = [], []\n",
        "\n",
        "# Iterate through the validation generator to collect all predictions and true labels.\n",
        "for batch_idx, (Xb, yb) in enumerate(val_gen):\n",
        "    # Optional: Check for NaN/Inf values in batch before prediction.\n",
        "    # Such values can cause model prediction errors.\n",
        "    if np.any(np.isnan(Xb)) or np.any(np.isinf(Xb)):\n",
        "        print(f\"Warning: Batch {batch_idx} contains NaN or Inf values. Skipping prediction for this batch.\")\n",
        "        continue # Skip this batch if it has problematic values\n",
        "\n",
        "    # Optional: Check if a batch is entirely zeros. This might indicate an issue\n",
        "    # with data loading or preprocessing, as silent audio might not be informative.\n",
        "    if np.all(Xb == 0):\n",
        "        print(f\"Warning: Batch {batch_idx} is all zeros. This might indicate an issue with data loading/preprocessing.\")\n",
        "\n",
        "    try:\n",
        "        # Predict on the current batch. `verbose=0` prevents progress bar for each batch prediction.\n",
        "        preds = model.predict(Xb, verbose=0)\n",
        "        # Convert predicted probabilities to class labels (index of the highest probability).\n",
        "        pred_labels = np.argmax(preds, axis=1)\n",
        "        # Extend the lists with predictions and true labels for the current batch.\n",
        "        val_preds.extend(pred_labels.tolist())\n",
        "        val_truth.extend(yb.tolist())\n",
        "    except Exception as e:\n",
        "        # Catch and report any errors during batch prediction.\n",
        "        print(f\"Error predicting on batch {batch_idx}: {e}\")\n",
        "        print(f\"Problematic batch shape: {Xb.shape}\")\n",
        "\n",
        "# Defensive check to avoid errors if no predictions were collected (e.g., due to all batches having NaNs).\n",
        "if val_truth and val_preds:\n",
        "    # Compute the confusion matrix.\n",
        "    cm = confusion_matrix(val_truth, val_preds, labels=list(range(num_classes)))\n",
        "    # Map class IDs back to their original string names for better readability in the confusion matrix.\n",
        "    label_names = [df[df['classID'] == i]['class'].values[0] for i in range(num_classes)]\n",
        "\n",
        "    # Plot the confusion matrix.\n",
        "    plt.figure(figsize=(10,8))\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_names)\n",
        "    disp.plot(xticks_rotation=45, cmap='magma', colorbar=True) # Rotate x-axis labels for readability\n",
        "    plt.title(\"Validation Confusion Matrix (Double UNet)\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Warning: No predictions available for confusion matrix. Check data loading and prediction loop.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "vhnG4e82GmtT"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}